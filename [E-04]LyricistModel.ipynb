{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30386c91",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f15e9f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 187088\n",
      "Examples:\n",
      " ['[Verse 1]', 'They come from everywhere', 'A longing to be free']\n"
     ]
    }
   ],
   "source": [
    "#데이터 읽어오기\n",
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"Data size:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "026973be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Verse 1]\n",
      "They come from everywhere\n",
      "A longing to be free\n",
      "They come to join us here\n",
      "From sea to shining sea And they all have a dream\n",
      "As people always will\n",
      "To be safe and warm\n",
      "In that shining city on the hill Some wanna slam the door\n",
      "Instead of opening the gate\n",
      "Aw, let's turn this thing around\n",
      "Before it gets too late [Chorus]\n",
      "It's up to me and you\n",
      "Love can conquer hate\n",
      "I know this to be true\n",
      "That's what makes us great [Verse 2]\n",
      "Don't tell me a lie\n"
     ]
    }
   ],
   "source": [
    "#데이터 정제\n",
    "\n",
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0:\n",
    "        continue\n",
    "    if sentence[-1] == \":\":\n",
    "        continue\n",
    "    if idx > 15: \n",
    "        break\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bb0eb7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#특수문자 제거\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()   \n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) #특수문자 양쪽 공백\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) #여러개의 공백은 하나의 공백으로.\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) #a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로.\n",
    "    sentence = sentence.strip()#양쪽 공백 지우기\n",
    "    sentence = '<start> ' + sentence + ' <end>'#<start>,<end>추가\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7d1a573c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> verse <end>',\n",
       " '<start> they come from everywhere <end>',\n",
       " '<start> a longing to be free <end>',\n",
       " '<start> they come to join us here <end>',\n",
       " '<start> from sea to shining sea and they all have a dream <end>',\n",
       " '<start> as people always will <end>',\n",
       " '<start> to be safe and warm <end>',\n",
       " '<start> in that shining city on the hill some wanna slam the door <end>',\n",
       " '<start> instead of opening the gate <end>',\n",
       " '<start> aw , let s turn this thing around <end>']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:#공백과 ':'제거\n",
    "    if len(sentence) == 0: \n",
    "        continue\n",
    "    if sentence[-1] == \":\":\n",
    "        continue\n",
    "    \n",
    "    corpus.append(preprocess_sentence(sentence))#특수문자제거\n",
    "\n",
    "corpus[:10]#10개만 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3bdf47",
   "metadata": {},
   "source": [
    "**Tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "72dbbf5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  520    3 ...    0    0    0]\n",
      " [   2   45   66 ...    0    0    0]\n",
      " [   2    9 3390 ...    0    0    0]\n",
      " ...\n",
      " [   2  561   21 ...    0    0    0]\n",
      " [   2  120   34 ...    0    0    0]\n",
      " [   2    5   22 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fa55843fe90>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=7000, #전체 단어의 개수\n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    total_data_text = list(tensor)\n",
    "    num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "    max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "    maxlen = int(max_tokens)#corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰짐  \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post',maxlen=maxlen)\n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3d13dbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5e7bf1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 520   3   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0]\n",
      "[520   3   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1] #마지막 토큰 잘라냄. <pad>일 가능성이 높다.\n",
    "tgt_input = tensor[:, 1:]#<start>잘라냄.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae61b37c",
   "metadata": {},
   "source": [
    "**Test data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b26fcf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
    "                                                          tgt_input,\n",
    "                                                          test_size=0.2,\n",
    "                                                          shuffle=True, \n",
    "                                                          random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d81f30cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train:  (140599, 19)\n",
      "Target Train:  (140599, 19)\n"
     ]
    }
   ],
   "source": [
    "#데이터 쉡 확인\n",
    "print('Source Train: ', enc_train.shape)\n",
    "print('Target Train: ', dec_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547aecc3",
   "metadata": {},
   "source": [
    "**인공지능 만들기**\n",
    "\n",
    "10epoch, target val_loss value 2.2 under\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4e8e51e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#텍스트 생성 모델 정의\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self,vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)#단어를 추상적으로 변환하는 역할\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)#문장을 순차적으로 읽으며 단어 간의 연관성을 분석\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)#RNN통해 나온 단어를 결정\n",
    "       \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 19 #언어의 백터화 (256은 너무 커서 19로 조정)\n",
    "hidden_size = 2048\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1f74cbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "550/550 [==============================] - 576s 1s/step - loss: 3.4444 - val_loss: 2.7176\n",
      "Epoch 2/7\n",
      "550/550 [==============================] - 557s 1s/step - loss: 2.6373 - val_loss: 2.4804\n",
      "Epoch 3/7\n",
      "550/550 [==============================] - 561s 1s/step - loss: 2.4318 - val_loss: 2.3530\n",
      "Epoch 4/7\n",
      "550/550 [==============================] - 561s 1s/step - loss: 2.2915 - val_loss: 2.2669\n",
      "Epoch 5/7\n",
      "550/550 [==============================] - 561s 1s/step - loss: 2.1881 - val_loss: 2.2052\n",
      "Epoch 6/7\n",
      "550/550 [==============================] - 561s 1s/step - loss: 2.0968 - val_loss: 2.1561\n",
      "Epoch 7/7\n",
      "550/550 [==============================] - 561s 1s/step - loss: 2.0051 - val_loss: 2.1072\n"
     ]
    }
   ],
   "source": [
    "#모델학습\n",
    "\n",
    "term = []\n",
    "epochs = 7\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "term = model.fit(enc_train, \n",
    "          dec_train, \n",
    "          epochs=epochs,\n",
    "          batch_size=256,\n",
    "          validation_data=(enc_val, dec_val),\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fbba8e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "590849ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you to love me <end> '"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5316dc6e",
   "metadata": {},
   "source": [
    "-정리-\n",
    "epoch 6 이상부터 val_loss가 2.2 이하로 떨어지기 시작했다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
