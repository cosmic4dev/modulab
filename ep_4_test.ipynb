{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "guided-saturn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 187088\n",
      "Examples:\n",
      " ['[Verse 1]', 'They come from everywhere', 'A longing to be free']\n"
     ]
    }
   ],
   "source": [
    "#데이터 읽어오기\n",
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"Data size:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "finite-devon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Verse 1]\n",
      "They come from everywhere\n",
      "A longing to be free\n",
      "They come to join us here\n",
      "From sea to shining sea And they all have a dream\n",
      "As people always will\n",
      "To be safe and warm\n",
      "In that shining city on the hill Some wanna slam the door\n",
      "Instead of opening the gate\n",
      "Aw, let's turn this thing around\n",
      "Before it gets too late [Chorus]\n",
      "It's up to me and you\n",
      "Love can conquer hate\n",
      "I know this to be true\n",
      "That's what makes us great [Verse 2]\n",
      "Don't tell me a lie\n"
     ]
    }
   ],
   "source": [
    "#데이터 정제\n",
    "\n",
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0:\n",
    "        continue\n",
    "    if sentence[-1] == \":\":\n",
    "        continue\n",
    "    if idx > 15: \n",
    "        break\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "indoor-arnold",
   "metadata": {},
   "outputs": [],
   "source": [
    "#특수문자 제거\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()   \n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) #특수문자 양쪽 공백\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) #여러개의 공백은 하나의 공백으로.\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) #a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로.\n",
    "    sentence = sentence.strip()#양쪽 공백 지우기\n",
    "    sentence = '<start> ' + sentence + ' <end>'#<start>,<end>추가\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "jewish-afternoon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> verse <end>',\n",
       " '<start> they come from everywhere <end>',\n",
       " '<start> a longing to be free <end>',\n",
       " '<start> they come to join us here <end>',\n",
       " '<start> from sea to shining sea and they all have a dream <end>',\n",
       " '<start> as people always will <end>',\n",
       " '<start> to be safe and warm <end>',\n",
       " '<start> in that shining city on the hill some wanna slam the door <end>',\n",
       " '<start> instead of opening the gate <end>',\n",
       " '<start> aw , let s turn this thing around <end>']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:#공백과 ':'제거\n",
    "    if len(sentence) == 0: \n",
    "        continue\n",
    "    if sentence[-1] == \":\":\n",
    "        continue\n",
    "    \n",
    "    corpus.append(preprocess_sentence(sentence))#특수문자제거\n",
    "\n",
    "corpus[:10]#10개만 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-journalism",
   "metadata": {},
   "source": [
    "**Tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "temporal-cambodia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  520    3 ...    0    0    0]\n",
      " [   2   45   66 ...    0    0    0]\n",
      " [   2    9 3390 ...    0    0    0]\n",
      " ...\n",
      " [   2  561   21 ...    0    0    0]\n",
      " [   2  120   34 ...    0    0    0]\n",
      " [   2    5   22 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fa55843ff50>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=7000, #전체 단어의 개수\n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    total_data_text = list(tensor)\n",
    "    num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "    max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "    maxlen = int(max_tokens)#corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰짐  \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post',maxlen=maxlen)\n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "minus-authority",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "alone-supervision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 520   3   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0]\n",
      "[520   3   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1] #마지막 토큰 잘라냄. <pad>일 가능성이 높다.\n",
    "tgt_input = tensor[:, 1:]#<start>잘라냄.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-single",
   "metadata": {},
   "source": [
    "**Test data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "humanitarian-secretary",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
    "                                                          tgt_input,\n",
    "                                                          test_size=0.2,\n",
    "                                                          shuffle=True, \n",
    "                                                          random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "civilian-employment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train:  (140599, 19)\n",
      "Target Train:  (140599, 19)\n"
     ]
    }
   ],
   "source": [
    "#데이터 쉡 확인\n",
    "print('Source Train: ', enc_train.shape)\n",
    "print('Target Train: ', dec_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-occasions",
   "metadata": {},
   "source": [
    "**인공지능 만들기**\n",
    "\n",
    "10epoch, target val_loss value 2.2 under\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "attended-attack",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 19), (256, 19)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BUFFER_SIZE = len(src_input)\n",
    "# BATCH_SIZE = 256\n",
    "# steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "#  # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "# VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# # 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
    "# # 데이터셋에 대해서는 아래 문서를 참고하세요\n",
    "# # 자세히 알아둘수록 도움이 많이 되는 중요한 문서입니다\n",
    "# # https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "# dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "# dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "formed-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "#텍스트 생성 모델 정의\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self,vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)#단어를 추상적으로 변환하는 역할\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)#문장을 순차적으로 읽으며 단어 간의 연관성을 분석\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)#RNN통해 나온 단어를 결정\n",
    "       \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 19 #언어의 백터화 (256은 너무 커서 19로 조정)\n",
    "hidden_size = 2048\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-packing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "143/550 [======>.......................] - ETA: 6:19 - loss: 2.4977"
     ]
    }
   ],
   "source": [
    "#모델학습\n",
    "\n",
    "term = []\n",
    "epochs = 10\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "term = model.fit(enc_train, \n",
    "          dec_train, \n",
    "          epochs=epochs,\n",
    "          batch_size=256,\n",
    "          validation_data=(enc_val, dec_val),\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-scholar",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-dryer",
   "metadata": {},
   "source": [
    "정리"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
